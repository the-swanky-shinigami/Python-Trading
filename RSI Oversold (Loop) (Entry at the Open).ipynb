{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"WbXW5bVMA8rI"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A2mmKX3nBJJj"},"outputs":[],"source":["# Function to read stock data from an Excel file\n","def get_stock_data_from_excel(file_path):\n","    all_stock_data = pd.read_excel(file_path, sheet_name=None)\n","    stock_data = {}\n","\n","    for sheet_name, data in all_stock_data.items():\n","        # Ensure 'Date' is treated as a datetime column and set as index\n","        data['Date'] = pd.to_datetime(data['Date'])\n","        data.set_index('Date', inplace=True)\n","        stock_data[sheet_name] = data\n","\n","    return stock_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XGp5ErdBKf_"},"outputs":[],"source":["def create_scrip_data(df, N, P):\n","    N_half = N // 2\n","\n","    # Initialize the Scrip_Data DataFrame with required columns\n","    scrip_data = pd.DataFrame(index=df.index)\n","    scrip_data['Close'] = df['Close']\n","    scrip_data['Open'] = df['Open']\n","    scrip_data['High'] = df['High']\n","    scrip_data['Low'] = df['Low']\n","    scrip_data['Volume'] = df['Volume']\n","\n","    # Calculate Change\n","    scrip_data['Change'] = scrip_data['Close'].diff()\n","\n","    # Calculate Gain and Loss\n","    scrip_data['Gain'] = np.where(scrip_data['Change'] > 0, scrip_data['Change'], 0)\n","    scrip_data['Loss'] = np.where(scrip_data['Gain'] == 0, -1 * scrip_data['Change'], 0)\n","\n","    # Calculate Average Gain and Average Loss\n","    scrip_data['Average Gain'] = pd.Series(np.nan, index=scrip_data.index)\n","    scrip_data['Average Loss'] = pd.Series(np.nan, index=scrip_data.index)\n","\n","    # Initial Average Gain and Loss for the first P days\n","    scrip_data.loc[scrip_data.index[P], 'Average Gain'] = scrip_data.loc[scrip_data.index[1:P+1], 'Gain'].mean()\n","    scrip_data.loc[scrip_data.index[P], 'Average Loss'] = scrip_data.loc[scrip_data.index[1:P+1], 'Loss'].mean()\n","\n","    # Copy columns to avoid SettingWithCopyWarning\n","    scrip_data['Average Gain'] = scrip_data['Average Gain'].copy()\n","    scrip_data['Average Loss'] = scrip_data['Average Loss'].copy()\n","\n","    # Calculate Average Gain and Average Loss using exponential smoothing formula\n","    for i in range(P + 1, len(scrip_data)):\n","        scrip_data.loc[scrip_data.index[i], 'Average Gain'] = ((scrip_data.loc[scrip_data.index[i - 1], 'Average Gain'] * (P - 1)) + scrip_data.loc[scrip_data.index[i], 'Gain']) / P\n","        scrip_data.loc[scrip_data.index[i], 'Average Loss'] = ((scrip_data.loc[scrip_data.index[i - 1], 'Average Loss'] * (P - 1)) + scrip_data.loc[scrip_data.index[i], 'Loss']) / P\n","\n","\n","    # Calculate Relative Strength (RS)\n","    scrip_data['Relative Strength'] = scrip_data['Average Gain'] / scrip_data['Average Loss']\n","\n","    # Calculate Relative Strength Index (RSI)\n","    scrip_data['RSI'] = 100 - (100 / (1 + scrip_data['Relative Strength']))\n","\n","    #Initialize columns for Oversold and Overbought Indicators\n","    scrip_data['Oversold Indicator'] = pd.Series(np.nan, index=scrip_data.index, dtype='object')\n","    #scrip_data['Overbought Indicator'] = pd.Series(np.nan, index=scrip_data.index, dtype='object')\n","\n","    # Initialize the Trade-related columns\n","    scrip_data[f'Trade taken in last {N_half} days'] = pd.NA\n","    scrip_data['Enter Trade'] = ''\n","    scrip_data['Trade Number'] = pd.NA\n","    scrip_data[f'{N_half}th Day'] = pd.NA\n","\n","    trade_number = 0\n","    trade_taken = [0] * len(scrip_data)\n","\n","    # Modify Trade signals and tracking logic\n","    for i in range(max(N, P + 1), len(scrip_data)):\n","        # Check the last N_half days for OB or OS signals\n","        #last_n_half_days_OB = scrip_data['Overbought Indicator'].iloc[i - N_half:i]\n","        last_n_half_days_OS = scrip_data['Oversold Indicator'].iloc[i - N_half:i]\n","\n","        last_n_half_days = scrip_data['Enter Trade'].iloc[i-N_half:i]\n","        trade_taken[i] = last_n_half_days[last_n_half_days == 'ENTER'].count()\n","\n","        # No OB or OS signal in the last N_half days\n","        #ob_signal_in_last_n_half_days = (last_n_half_days_OB == 'OB').any()\n","        os_signal_in_last_n_half_days = (last_n_half_days_OS == 'OS').any()\n","\n","        # Generate Oversold and Overbought indicators\n","        if scrip_data['RSI'].iloc[i] < 30:\n","            scrip_data.loc[scrip_data.index[i], 'Oversold Indicator'] = 'OS'\n","        else:\n","            scrip_data.loc[scrip_data.index[i], 'Oversold Indicator'] = 'NA'\n","\n","        #if scrip_data['RSI'].iloc[i] > 70:\n","        #    scrip_data.loc[scrip_data.index[i], 'Overbought Indicator'] = 'OB'\n","        #else:\n","        #    scrip_data.loc[scrip_data.index[i], 'Overbought Indicator'] = 'NA'\n","\n","        # Enter trade logic: Enter only if there is no OB or OS signal in the last N_half days\n","        if scrip_data['Oversold Indicator'].iloc[i] == 'OS' and not os_signal_in_last_n_half_days:\n","            scrip_data.loc[scrip_data.index[i], 'Enter Trade'] = 'ENTER'\n","            trade_number += 1\n","        else:\n","            scrip_data.loc[scrip_data.index[i], 'Enter Trade'] = 'NA'\n","\n","        scrip_data.loc[scrip_data.index[i], 'Trade Number'] = trade_number\n","        scrip_data.loc[scrip_data.index[i], f'Trade taken in last {N_half} days'] = trade_taken[i]\n","\n","        # Set the N//2th Day column\n","        if i + N_half - 1 < len(scrip_data):\n","                scrip_data.loc[scrip_data.index[i], f'{N_half}th Day'] = scrip_data.index[i + N_half - 1]\n","\n","        # Update the trade_taken list\n","        if scrip_data.loc[scrip_data.index[i], 'Enter Trade'] == 'ENTER':\n","            trade_taken[i] = 1\n","        else:\n","            trade_taken[i] = 0\n","\n","    return scrip_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCtNqQuzBPH8"},"outputs":[],"source":["def create_trade_data(scrip_data, N):\n","    trade_data = []\n","    N_half = N // 2\n","    trades = scrip_data[scrip_data['Enter Trade'] == 'ENTER']\n","\n","    for index, trade in trades.iterrows():\n","        trade_number = trade['Trade Number']\n","        entry_day = index\n","        nth_day = trade[f'{N_half}th Day']\n","        entry_price = trade['Close']\n","\n","        # Adjust entry to be at the open of the next day\n","        next_day_index = scrip_data.index.get_loc(entry_day) + 1\n","        if next_day_index >= len(scrip_data):\n","            continue  # Skip if the next day is out of bounds\n","\n","        next_day_data = scrip_data.iloc[next_day_index]\n","        entry_day = next_day_data.name  # Update entry_day to the next day\n","        entry_price = next_day_data['Open']\n","\n","        # Calculate the N/2th day based on the adjusted entry day\n","        nth_day_index = next_day_index + (N_half - 1)\n","        if nth_day_index >= len(scrip_data):\n","            continue  # Skip if N_halfth day is out of bounds\n","\n","        nth_day = scrip_data.index[nth_day_index]\n","\n","        # Include the high for both entry and exit days in the calculation\n","        trade_period = scrip_data.loc[entry_day:nth_day]\n","        highest_point = max(trade_period['High'].max(), next_day_data['High'])\n","        lowest_point = min(trade_period['Low'].min(), next_day_data['Low'])\n","\n","        highest_point_date = trade_period['High'].idxmax()\n","        lowest_point_date = trade_period['Low'].idxmin()\n","\n","        long_potential = ((highest_point - entry_price) / entry_price) * 100\n","        short_potential = ((entry_price - lowest_point) / entry_price) * 100\n","\n","        whipsaw_long = 'Yes' if long_potential < 0 else 'No'\n","        whipsaw_short = 'Yes' if short_potential < 0 else 'No'\n","\n","        reached_first = 'Lowest' if lowest_point_date < highest_point_date else 'Highest'\n","\n","        trade_data.append({\n","            'Trade Number': trade_number,\n","            'Entry Day': entry_day,  # Now reflects the next day\n","            f'{N_half}th Day': nth_day,  # Adjusted N_halfth day\n","            'Entry Price': entry_price,\n","            'Highest Point': highest_point,\n","            'Lowest Point': lowest_point,\n","            'Long Potential': long_potential,\n","            'Short Potential': short_potential,\n","            'Whipsaw Long': whipsaw_long,\n","            'Whipsaw Short': whipsaw_short,\n","            'Date of Highest Point': highest_point_date,\n","            'Date of Lowest Point': lowest_point_date,\n","            'Reached First?': reached_first\n","        })\n","\n","    trade_df = pd.DataFrame(trade_data)\n","\n","    return trade_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpmNVbcIBRuO"},"outputs":[],"source":["def create_analysis(trade_data, N):\n","    N_half = N // 2\n","\n","    # Calculate required metrics for the Analysis dataframe\n","    total_trades = len(trade_data)\n","\n","    long_potential_30th_percentile = trade_data['Long Potential'].quantile(0.30)\n","    short_potential_30th_percentile = trade_data['Short Potential'].quantile(0.30)\n","\n","    long_whipsaws_count = trade_data['Whipsaw Long'].value_counts().get('Yes', 0)\n","    short_whipsaws_count = trade_data['Whipsaw Short'].value_counts().get('Yes', 0)\n","\n","    short_reached_first = trade_data['Reached First?'].value_counts().get('Highest', 0)\n","    long_reached_first = trade_data['Reached First?'].value_counts().get('Lowest', 0)\n","\n","    # Create a dictionary to store the analysis data\n","    analysis_data = {\n","        'LookBack Period': N,\n","        'N/2': N_half,\n","        'Total Trades': total_trades,\n","        '30th Percentile Long Potential': long_potential_30th_percentile,\n","        '30th Percentile Short Potential': short_potential_30th_percentile,\n","        'Long Whipsaws Count': long_whipsaws_count,\n","        'Short Whipsaws Count': short_whipsaws_count,\n","        'Short Reached First': short_reached_first,\n","        'Long Reached First': long_reached_first\n","    }\n","\n","    # Convert the dictionary to a DataFrame\n","    analysis_df = pd.DataFrame([analysis_data])\n","\n","    return analysis_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGJfOQ1LBVqn"},"outputs":[],"source":["# Path to the Excel file containing stock data\n","file_path = '/content/drive/MyDrive/Data/DATA_FOR_TESTING.xlsx'\n","output_file_path = '/content/drive/MyDrive/Analysis/Relative Strength Index (Entry at the Open)/AllStocks_Analysis_RSI_Oversold.xlsx'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aEbaHgKECkJ_"},"outputs":[],"source":["# Get stock data from Excel file\n","stock_data = get_stock_data_from_excel(file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GnI2MZ7gCmmb","executionInfo":{"status":"ok","timestamp":1724411804576,"user_tz":-330,"elapsed":1298312,"user":{"displayName":"Saumya Dwivedi","userId":"06991737587028914304"}},"outputId":"70a7e155-c8cc-439d-bb42-dce53e9a2ccd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing AARTIIND...\n","Processing ABB...\n","Processing ABBOTINDIA...\n","Processing ABCAPITAL...\n","Processing ABFRL...\n","Processing ACC...\n","Processing ADANIENT...\n","Processing ADANIPORTS...\n","Processing ALKEM...\n","Processing AMBUJACEM...\n","Processing APOLLOHOSP...\n","Processing APOLLOTYRE...\n","Processing ASHOKLEY...\n","Processing ASIANPAINT...\n","Processing ASTRAL...\n","Processing ATUL...\n","Processing AUBANK...\n","Processing AUROPHARMA...\n","Processing AXISBANK...\n","Processing BAJAJ-AUTO...\n","Processing BAJAJFINSV...\n","Processing BAJFINANCE...\n","Processing BALKRISIND...\n","Processing BALRAMCHIN...\n","Processing BANDHANBNK...\n","Processing BANKBARODA...\n","Processing BATAINDIA...\n","Processing BEL...\n","Processing BERGEPAINT...\n","Processing BHARATFORG...\n","Processing BHARTIARTL...\n","Processing BHEL...\n","Processing BIOCON...\n","Processing BOSCHLTD...\n","Processing BPCL...\n","Processing BRITANNIA...\n","Processing BSOFT...\n","Processing CANBK...\n","Processing CANFINHOME...\n","Processing CHAMBLFERT...\n","Processing CHOLAFIN...\n","Processing CIPLA...\n","Processing COALINDIA...\n","Processing COFORGE...\n","Processing COLPAL...\n","Processing CONCOR...\n","Processing COROMANDEL...\n","Processing CROMPTON...\n","Processing CUB...\n","Processing CUMMINSIND...\n","Processing DABUR...\n","Processing DALBHARAT...\n","Processing DEEPAKNTR...\n","Processing DIVISLAB...\n","Processing DIXON...\n","Processing DLF...\n","Processing DRREDDY...\n","Processing EICHERMOT...\n","Processing ESCORTS...\n","Processing EXIDEIND...\n","Processing FEDERALBNK...\n","Processing GAIL...\n","Processing GLENMARK...\n","Processing GMRINFRA...\n","Processing GNFC...\n","Processing GODREJCP...\n","Processing GODREJPROP...\n","Processing GRANULES...\n","Processing GRASIM...\n","Processing GUJGASLTD...\n","Processing HAL...\n","Processing HAVELLS...\n","Processing HCLTECH...\n","Processing HDFCAMC...\n","Processing HDFCBANK...\n","Processing HDFCLIFE...\n","Processing HEROMOTOCO...\n","Processing HINDALCO...\n","Processing HINDCOPPER...\n","Processing HINDPETRO...\n","Processing HINDUNILVR...\n","Processing ICICIBANK...\n","Processing ICICIGI...\n","Processing ICICIPRULI...\n","Processing IDEA...\n","Processing IDFC...\n","Processing IDFCFIRSTB...\n","Processing IEX...\n","Processing IGL...\n","Processing INDHOTEL...\n","Processing INDIACEM...\n","Processing INDIAMART...\n","Processing INDIGO...\n","Processing INDUSINDBK...\n","Processing INDUSTOWER...\n","Processing INFY...\n","Processing IOC...\n","Processing IPCALAB...\n","Processing IRCTC...\n","Processing ITC...\n","Processing JINDALSTEL...\n","Processing JKCEMENT...\n","Processing JSWSTEEL...\n","Processing JUBLFOOD...\n","Processing KOTAKBANK...\n","Processing LALPATHLAB...\n","Processing LAURUSLABS...\n","Processing LICHSGFIN...\n","Processing LT...\n","Processing LTIM...\n","Processing LTTS...\n","Processing LUPIN...\n","Processing M&M...\n","Processing M&MFIN...\n","Processing MANAPPURAM...\n","Processing MARICO...\n","Processing MARUTI...\n","Processing MCDOWELL-N...\n","Processing MCX...\n","Processing METROPOLIS...\n","Processing MFSL...\n","Processing MGL...\n","Processing MOTHERSON...\n","Processing MPHASIS...\n","Processing MRF...\n","Processing MUTHOOTFIN...\n","Processing NATIONALUM...\n","Processing NAUKRI...\n","Processing NAVINFLUOR...\n","Processing NESTLEIND...\n","Processing NMDC...\n","Processing NTPC...\n","Processing OBEROIRLTY...\n","Processing OFSS...\n","Processing ONGC...\n","Processing PAGEIND...\n","Processing PEL...\n","Processing PERSISTENT...\n","Processing PETRONET...\n","Processing PFC...\n","Processing PIDILITIND...\n","Processing PIIND...\n","Processing PNB...\n","Processing POLYCAB...\n","Processing POWERGRID...\n","Processing PVRINOX...\n","Processing RAMCOCEM...\n","Processing RBLBANK...\n","Processing RECLTD...\n","Processing RELIANCE...\n","Processing SAIL...\n","Processing SBICARD...\n","Processing SBILIFE...\n","Processing SBIN...\n","Processing SHREECEM...\n","Processing SHRIRAMFIN...\n","Processing SIEMENS...\n","Processing SRF...\n","Processing SUNPHARMA...\n","Processing SUNTV...\n","Processing SYNGENE...\n","Processing TATACHEM...\n","Processing TATACOMM...\n","Processing TATACONSUM...\n","Processing TATAMOTORS...\n","Processing TATAPOWER...\n","Processing TATASTEEL...\n","Processing TCS...\n","Processing TECHM...\n","Processing TITAN...\n","Processing TORNTPHARM...\n","Processing TRENT...\n","Processing TVSMOTOR...\n","Processing UBL...\n","Processing ULTRACEMCO...\n","Processing UPL...\n","Processing VEDL...\n","Processing VOLTAS...\n","Processing WIPRO...\n","Processing ZEEL...\n","Processing ZYDUSLIFE...\n"]}],"source":["with pd.ExcelWriter(output_file_path) as writer:\n","    for scrip, df in stock_data.items():\n","        print(f\"Processing {scrip}...\")\n","        # Create an Excel sheet for each scrip\n","        analysis_results = []\n","\n","        for N in range(5, 51):  # Looping N from 5 to 50\n","            scrip_data = create_scrip_data(df, N=N, P=14)\n","            trade_data = create_trade_data(scrip_data, N=N)\n","            analysis_data = create_analysis(trade_data, N=N)\n","            analysis_results.append(analysis_data)\n","\n","        # Combine all analysis results for current scrip into a single DataFrame\n","        combined_analysis_df = pd.concat(analysis_results, ignore_index=True)\n","\n","        # Save the combined analysis results for the current scrip\n","        combined_analysis_df.to_excel(writer, sheet_name=scrip, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6iaNNibDSv9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724411804591,"user_tz":-330,"elapsed":14,"user":{"displayName":"Saumya Dwivedi","userId":"06991737587028914304"}},"outputId":"410ef112-0340-4ac8-baa0-530ab84ebfa5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processed data saved to /content/drive/MyDrive/Analysis/Relative Strength Index (Entry at the Open)/AllStocks_Analysis_RSI_Oversold.xlsx\n"]}],"source":["print(f\"Processed data saved to {output_file_path}\")"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1ZZZygyP0gxgSW1_4nhII3s5haM9If3bd","authorship_tag":"ABX9TyN2fjcqG/O5SMcKiEaJCL+f"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}